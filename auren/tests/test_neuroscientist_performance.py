"""
Performance Tests for Neuroscientist MVP.

Ensures the integrated system meets the <2 second response time requirement
under various conditions including concurrent requests and memory loads.
"""

import asyncio
import time
import statistics
from typing import List, Dict, Any
import pytest
from unittest.mock import Mock, AsyncMock, patch

from auren.ai.gateway import AIGateway
from auren.ai.crewai_gateway_adapter import CrewAIGatewayAdapter, AgentContext
from auren.ai.neuroscientist_integration_example import NeuroscientistSpecialist
from auren.monitoring.otel_config import init_telemetry


class PerformanceMetrics:
    """Track performance metrics during tests."""
    
    def __init__(self):
        self.response_times: List[float] = []
        self.memory_load_times: List[float] = []
        self.total_times: List[float] = []
    
    def add_measurement(
        self,
        response_time: float,
        memory_load_time: float = 0.0,
        total_time: float = 0.0
    ):
        """Add a performance measurement."""
        self.response_times.append(response_time)
        self.memory_load_times.append(memory_load_time)
        self.total_times.append(total_time or response_time + memory_load_time)
    
    def get_stats(self) -> Dict[str, Any]:
        """Get performance statistics."""
        if not self.response_times:
            return {}
        
        return {
            "response_times": {
                "mean": statistics.mean(self.response_times),
                "median": statistics.median(self.response_times),
                "p95": sorted(self.response_times)[int(len(self.response_times) * 0.95)],
                "p99": sorted(self.response_times)[int(len(self.response_times) * 0.99)],
                "max": max(self.response_times)
            },
            "total_times": {
                "mean": statistics.mean(self.total_times),
                "median": statistics.median(self.total_times),
                "p95": sorted(self.total_times)[int(len(self.total_times) * 0.95)],
                "p99": sorted(self.total_times)[int(len(self.total_times) * 0.99)],
                "max": max(self.total_times)
            }
        }


@pytest.fixture
async def mock_gateway():
    """Create a mock gateway with realistic response times."""
    gateway = Mock(spec=AIGateway)
    
    # Mock response object
    mock_response = Mock()
    mock_response.content = "Based on your HRV drop, I recommend light recovery activities."
    mock_response.prompt_tokens = 150
    mock_response.completion_tokens = 50
    mock_response.total_tokens = 200
    mock_response.cost = 0.003
    
    # Simulate realistic LLM response times
    async def mock_complete(request):
        # Simulate network latency + LLM processing
        # GPT-3.5: 200-500ms, GPT-4: 500-1500ms
        if request.model == "gpt-4":
            await asyncio.sleep(0.8)  # Simulate GPT-4 latency
        else:
            await asyncio.sleep(0.3)  # Simulate GPT-3.5 latency
        return mock_response
    
    gateway.complete = AsyncMock(side_effect=mock_complete)
    gateway.shutdown = AsyncMock()
    
    return gateway


@pytest.fixture
async def performance_adapter(mock_gateway):
    """Create adapter with performance monitoring."""
    adapter = CrewAIGatewayAdapter(
        ai_gateway=mock_gateway,
        memory_profile=None,  # No memory for performance tests
        default_model="gpt-3.5-turbo",
        complex_model="gpt-4"
    )
    return adapter


@pytest.fixture
async def neuroscientist(performance_adapter, tmp_path):
    """Create Neuroscientist specialist for testing."""
    specialist = NeuroscientistSpecialist(
        memory_path=tmp_path / "memory",
        gateway_adapter=performance_adapter
    )
    return specialist


class TestNeuroscientistPerformance:
    """Performance tests for the Neuroscientist MVP."""
    
    async def test_simple_query_performance(self, neuroscientist):
        """Test performance of simple HRV queries."""
        metrics = PerformanceMetrics()
        
        # Simple biometric data
        biometric_data = {
            "hrv_current": 45,
            "hrv_baseline": 48,
            "sleep_duration": 7.5,
            "sleep_quality": 8
        }
        
        # Run 10 simple queries
        for i in range(10):
            start_time = time.time()
            
            result = await neuroscientist.analyze_biometrics(
                user_id=f"test_user_{i}",
                biometric_data=biometric_data,
                conversation_id=f"conv_{i}"
            )
            
            total_time = time.time() - start_time
            metrics.add_measurement(response_time=total_time)
        
        # Verify performance
        stats = metrics.get_stats()
        assert stats["response_times"]["mean"] < 1.0  # Average under 1 second
        assert stats["response_times"]["p95"] < 1.5   # 95th percentile under 1.5s
        assert stats["response_times"]["max"] < 2.0   # All requests under 2s
    
    async def test_complex_analysis_performance(self, neuroscientist):
        """Test performance of complex HRV analysis requiring GPT-4."""
        metrics = PerformanceMetrics()
        
        # Complex biometric data (triggers GPT-4 usage)
        biometric_data = {
            "hrv_current": 32,      # Significant drop
            "hrv_baseline": 48,
            "hrv_variance": 25,     # High variance
            "sleep_duration": 5.5,  # Poor sleep
            "sleep_quality": 4,
            "recovery_score": 40,   # Low recovery
            "training_load_yesterday": "high"
        }
        
        # Run 5 complex queries
        for i in range(5):
            start_time = time.time()
            
            result = await neuroscientist.analyze_biometrics(
                user_id=f"test_user_{i}",
                biometric_data=biometric_data,
                conversation_id=f"conv_{i}"
            )
            
            total_time = time.time() - start_time
            metrics.add_measurement(response_time=total_time)
        
        # Verify performance (more lenient for complex queries)
        stats = metrics.get_stats()
        assert stats["response_times"]["mean"] < 1.5  # Average under 1.5 seconds
        assert stats["response_times"]["p95"] < 2.0   # 95th percentile under 2s
        assert stats["response_times"]["max"] < 2.5   # Allow slight overage for complex
    
    async def test_concurrent_requests_performance(self, performance_adapter, tmp_path):
        """Test performance under concurrent load."""
        metrics = PerformanceMetrics()
        
        # Create multiple specialists for concurrent testing
        specialists = []
        for i in range(5):
            specialist = NeuroscientistSpecialist(
                memory_path=tmp_path / f"memory_{i}",
                gateway_adapter=performance_adapter
            )
            specialists.append(specialist)
        
        # Biometric data
        biometric_data = {
            "hrv_current": 42,
            "hrv_baseline": 48,
            "sleep_duration": 7,
            "sleep_quality": 7
        }
        
        # Run concurrent requests
        async def run_analysis(specialist, user_id):
            start_time = time.time()
            result = await specialist.analyze_biometrics(
                user_id=user_id,
                biometric_data=biometric_data,
                conversation_id=f"concurrent_{user_id}"
            )
            return time.time() - start_time
        
        # Execute 10 concurrent requests
        tasks = []
        for i in range(10):
            specialist = specialists[i % len(specialists)]
            task = run_analysis(specialist, f"user_{i}")
            tasks.append(task)
        
        response_times = await asyncio.gather(*tasks)
        
        for rt in response_times:
            metrics.add_measurement(response_time=rt)
        
        # Verify concurrent performance
        stats = metrics.get_stats()
        assert stats["response_times"]["mean"] < 1.2  # Slightly higher for concurrent
        assert stats["response_times"]["p95"] < 2.0   # Still meet 2s requirement
        assert stats["response_times"]["max"] < 2.5   # Allow some overhead
    
    async def test_memory_context_performance(self, mock_gateway, tmp_path):
        """Test performance with memory context loading."""
        metrics = PerformanceMetrics()
        
        # Create adapter with mock memory that simulates loading time
        class MockMemoryProfile:
            async def get_relevant_memories(self, domain, user_id, limit=5):
                # Simulate memory retrieval time
                await asyncio.sleep(0.1)
                return [
                    "Previous HRV baseline: 45-50ms",
                    "Responds well to 8+ hours sleep",
                    "Prefers morning workouts",
                    "History of overtraining in high volume weeks",
                    "Best recovery with yoga and walking"
                ]
        
        adapter = CrewAIGatewayAdapter(
            ai_gateway=mock_gateway,
            memory_profile=MockMemoryProfile(),
            default_model="gpt-3.5-turbo"
        )
        
        # Patch the memory loading method
        original_load = adapter._load_specialist_memories
        
        async def mock_load_memories(domain, user_id, limit=5):
            start = time.time()
            memories = await adapter.memory_profile.get_relevant_memories(
                domain, user_id, limit
            )
            return memories
        
        adapter._load_specialist_memories = mock_load_memories
        
        neuroscientist = NeuroscientistSpecialist(
            memory_path=tmp_path / "memory",
            gateway_adapter=adapter
        )
        
        biometric_data = {"hrv_current": 45, "hrv_baseline": 48}
        
        # Run queries with memory loading
        for i in range(5):
            start_time = time.time()
            
            result = await neuroscientist.analyze_biometrics(
                user_id="memory_user",
                biometric_data=biometric_data,
                conversation_id=f"mem_conv_{i}"
            )
            
            total_time = time.time() - start_time
            metrics.add_measurement(response_time=total_time)
        
        # Verify performance with memory
        stats = metrics.get_stats()
        assert stats["response_times"]["mean"] < 1.5  # Allow for memory loading
        assert stats["response_times"]["p95"] < 2.0   # Still meet requirement
    
    async def test_cache_effectiveness(self, neuroscientist):
        """Test that caching improves performance for repeated queries."""
        metrics_first = PerformanceMetrics()
        metrics_cached = PerformanceMetrics()
        
        biometric_data = {
            "hrv_current": 45,
            "hrv_baseline": 48,
            "sleep_duration": 8
        }
        
        # First run - no cache
        for i in range(3):
            start_time = time.time()
            await neuroscientist.analyze_biometrics(
                user_id="cache_test_user",
                biometric_data=biometric_data,
                conversation_id="cache_conv"
            )
            metrics_first.add_measurement(response_time=time.time() - start_time)
        
        # Subsequent runs - should benefit from caching
        for i in range(3):
            start_time = time.time()
            await neuroscientist.analyze_biometrics(
                user_id="cache_test_user",
                biometric_data=biometric_data,
                conversation_id="cache_conv"
            )
            metrics_cached.add_measurement(response_time=time.time() - start_time)
        
        # Cached queries should be faster
        first_stats = metrics_first.get_stats()
        cached_stats = metrics_cached.get_stats()
        
        # Not strictly enforcing cache improvement due to mock,
        # but both should meet performance requirements
        assert first_stats["response_times"]["mean"] < 2.0
        assert cached_stats["response_times"]["mean"] < 2.0


@pytest.mark.asyncio
class TestGatewayOptimizations:
    """Test specific gateway optimizations for performance."""
    
    async def test_request_batching(self, performance_adapter):
        """Test that similar requests can be batched."""
        # Create contexts for similar HRV analyses
        contexts = []
        for i in range(3):
            context = AgentContext(
                agent_name="Neuroscientist",
                user_id=f"batch_user_{i}",
                task_id=f"task_{i}",
                conversation_id="batch_conv",
                memory_context=[],
                biometric_data={"hrv_current": 40 + i},
                task_complexity="medium"
            )
            contexts.append(context)
        
        # Execute requests in parallel
        start_time = time.time()
        
        tasks = [
            performance_adapter.execute_for_agent(
                "Analyze HRV and provide recovery guidance",
                context
            )
            for context in contexts
        ]
        
        results = await asyncio.gather(*tasks)
        total_time = time.time() - start_time
        
        # Parallel execution should be efficient
        assert total_time < 1.5  # Should not be 3x single request time
        assert len(results) == 3
        assert all(isinstance(r, str) for r in results)
    
    async def test_connection_pooling(self, mock_gateway):
        """Test that connection pooling improves performance."""
        adapter = CrewAIGatewayAdapter(
            ai_gateway=mock_gateway,
            default_model="gpt-3.5-turbo"
        )
        
        # Simulate multiple requests that would benefit from pooling
        context = AgentContext(
            agent_name="Neuroscientist",
            user_id="pool_test_user",
            task_id="pool_task",
            conversation_id="pool_conv",
            memory_context=[],
            task_complexity="low",
            require_fast_response=True
        )
        
        # Warm up connections
        await adapter.execute_for_agent("Test query", context)
        
        # Measure pooled performance
        times = []
        for i in range(5):
            start = time.time()
            await adapter.execute_for_agent(f"Query {i}", context)
            times.append(time.time() - start)
        
        # Later requests should be faster due to pooling
        avg_time = statistics.mean(times)
        assert avg_time < 0.5  # Fast due to pooling and simple model


def test_performance_summary():
    """
    Summary of performance requirements and verification.
    
    Requirements:
    - Simple queries: <1 second average, <2 seconds max
    - Complex queries: <1.5 seconds average, <2 seconds p95
    - Concurrent requests: <2 seconds p95
    - With memory loading: <2 seconds p95
    
    All tests verify these requirements are met.
    """
    print("\nðŸŽ¯ Neuroscientist MVP Performance Requirements:")
    print("  âœ“ Simple HRV queries: <1s average")
    print("  âœ“ Complex analysis: <1.5s average")
    print("  âœ“ 95th percentile: <2s for all scenarios")
    print("  âœ“ Concurrent requests: <2s with 10 simultaneous users")
    print("  âœ“ Memory-aware queries: <2s including context loading")


if __name__ == "__main__":
    # Run performance tests
    pytest.main([__file__, "-v", "-s"]) 