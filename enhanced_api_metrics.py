# =============================================================================
# AUREN ENHANCED METRICS IMPLEMENTATION
# =============================================================================
# Complete custom metrics for webhook tracking, memory tier operations,
# NEUROS mode tracking, and biometric event processing
# =============================================================================

from prometheus_client import Counter, Histogram, Gauge, Info, Enum
import hashlib
import time
import json
from datetime import datetime
from typing import Dict, Any, Optional

# =============================================================================
# WEBHOOK METRICS
# =============================================================================

webhook_requests_total = Counter(
    'auren_webhook_requests_total',
    'Total number of webhook requests received',
    ['device_type', 'event_type', 'status']
)

webhook_request_duration_seconds = Histogram(
    'auren_webhook_request_duration_seconds',
    'Webhook request duration in seconds',
    ['device_type', 'method'],
    buckets=(0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0)
)

webhook_errors_total = Counter(
    'auren_webhook_errors_total',
    'Total number of webhook processing errors',
    ['device_type', 'error_type']
)

webhook_active_requests = Gauge(
    'auren_webhook_active_requests',
    'Number of webhook requests currently being processed',
    ['device_type']
)

webhook_payload_size_bytes = Histogram(
    'auren_webhook_payload_size_bytes',
    'Size of webhook payloads in bytes',
    ['device_type'],
    buckets=(100, 500, 1000, 5000, 10000, 50000, 100000)
)

# =============================================================================
# MEMORY TIER METRICS
# =============================================================================

memory_tier_operations_total = Counter(
    'auren_memory_tier_operations_total',
    'Total memory tier operations by the AI agent',
    ['tier', 'operation', 'trigger', 'success']
)

memory_tier_size_bytes = Gauge(
    'auren_memory_tier_size_bytes',
    'Current size of each memory tier in bytes',
    ['tier']
)

memory_tier_items_count = Gauge(
    'auren_memory_tier_items_count',
    'Number of items in each memory tier',
    ['tier']
)

memory_tier_latency_seconds = Histogram(
    'auren_memory_tier_latency_seconds',
    'Latency of memory tier operations',
    ['tier', 'operation'],
    buckets=(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 2.5)
)

memory_tier_hit_rate = Gauge(
    'auren_memory_tier_hit_rate',
    'Cache hit rate for memory tiers',
    ['tier']
)

ai_agent_decisions_total = Counter(
    'auren_ai_agent_decisions_total',
    'Total decisions made by NEUROS AI agent',
    ['decision_type', 'from_tier', 'to_tier', 'reason']
)

memory_promotion_duration_seconds = Histogram(
    'auren_memory_promotion_duration_seconds',
    'Time taken to promote data between tiers',
    ['from_tier', 'to_tier'],
    buckets=(0.01, 0.05, 0.1, 0.5, 1.0, 2.5, 5.0)
)

# =============================================================================
# NEUROS MODE METRICS
# =============================================================================

neuros_mode_switches_total = Counter(
    'auren_neuros_mode_switches_total',
    'Total NEUROS cognitive mode switches',
    ['from_mode', 'to_mode', 'trigger_type', 'user_id_hash']
)

# Using Enum for current mode tracking per user
neuros_current_mode = Enum(
    'auren_neuros_current_mode',
    'Current NEUROS cognitive mode',
    ['user_id_hash'],
    states=['baseline', 'reflex', 'hypothesis', 'pattern', 'companion', 'sentinel']
)

neuros_mode_duration_seconds = Histogram(
    'auren_neuros_mode_duration_seconds',
    'Duration spent in each NEUROS mode',
    ['mode', 'user_id_hash'],
    buckets=(60, 300, 600, 1800, 3600, 7200, 14400, 28800)  # 1m to 8h
)

neuros_hypothesis_generated_total = Counter(
    'auren_neuros_hypothesis_generated_total',
    'Total hypotheses generated by NEUROS',
    ['category', 'confidence_level']
)

neuros_pattern_recognition_total = Counter(
    'auren_neuros_pattern_recognition_total',
    'Patterns recognized by NEUROS',
    ['pattern_type', 'significance']
)

# =============================================================================
# BIOMETRIC EVENT METRICS
# =============================================================================

biometric_events_processed_total = Counter(
    'auren_biometric_events_processed_total',
    'Total biometric events processed',
    ['device_type', 'event_type', 'user_id_hash']
)

biometric_event_values = Histogram(
    'auren_biometric_event_values',
    'Distribution of biometric values',
    ['device_type', 'metric_type', 'unit'],
    buckets=(20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 140, 160, 180, 200)
)

biometric_anomalies_detected_total = Counter(
    'auren_biometric_anomalies_detected_total',
    'Anomalies detected in biometric data',
    ['device_type', 'anomaly_type', 'severity']
)

biometric_data_quality_score = Gauge(
    'auren_biometric_data_quality_score',
    'Quality score of biometric data (0-1)',
    ['device_type', 'user_id_hash']
)

# =============================================================================
# KAFKA & STREAMING METRICS
# =============================================================================

kafka_messages_sent_total = Counter(
    'auren_kafka_messages_sent_total',
    'Total messages sent to Kafka',
    ['topic', 'status']
)

kafka_message_send_duration_seconds = Histogram(
    'auren_kafka_message_send_duration_seconds',
    'Time to send message to Kafka',
    ['topic'],
    buckets=(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0)
)

kafka_batch_size = Histogram(
    'auren_kafka_batch_size',
    'Size of Kafka message batches',
    ['topic'],
    buckets=(1, 5, 10, 50, 100, 500, 1000)
)

# =============================================================================
# SYSTEM HEALTH METRICS
# =============================================================================

system_info = Info(
    'auren_system',
    'AUREN system information'
)

database_connections_active = Gauge(
    'auren_database_connections_active',
    'Active database connections',
    ['database', 'pool_name']
)

api_endpoint_availability = Gauge(
    'auren_api_endpoint_availability',
    'API endpoint availability (0=down, 1=up)',
    ['endpoint', 'method']
)

# =============================================================================
# HELPER FUNCTIONS WITH METRICS
# =============================================================================

class MetricsTracker:
    """Enhanced metrics tracking with context management"""
    
    def __init__(self):
        self.mode_start_times = {}
    
    def track_webhook_request(self, device_type: str, event_type: str, 
                            start_time: float, status: str = "success", 
                            error: Optional[str] = None, payload_size: int = 0):
        """Track comprehensive webhook metrics"""
        duration = time.time() - start_time
        
        webhook_requests_total.labels(
            device_type=device_type,
            event_type=event_type,
            status=status
        ).inc()
        
        webhook_request_duration_seconds.labels(
            device_type=device_type,
            method="POST"
        ).observe(duration)
        
        if payload_size > 0:
            webhook_payload_size_bytes.labels(
                device_type=device_type
            ).observe(payload_size)
        
        if error:
            webhook_errors_total.labels(
                device_type=device_type,
                error_type=error
            ).inc()
    
    def track_memory_operation(self, tier: str, operation: str, 
                             trigger: str, success: bool, 
                             duration: float, size_bytes: Optional[int] = None,
                             item_count: Optional[int] = None):
        """Track memory tier operations"""
        memory_tier_operations_total.labels(
            tier=tier,
            operation=operation,
            trigger=trigger,
            success=str(success)
        ).inc()
        
        memory_tier_latency_seconds.labels(
            tier=tier,
            operation=operation
        ).observe(duration)
        
        if size_bytes is not None:
            if operation == "add":
                memory_tier_size_bytes.labels(tier=tier).inc(size_bytes)
            elif operation == "remove":
                memory_tier_size_bytes.labels(tier=tier).dec(size_bytes)
        
        if item_count is not None:
            memory_tier_items_count.labels(tier=tier).set(item_count)
    
    def track_memory_promotion(self, user_id: str, from_tier: str, 
                             to_tier: str, size_bytes: int, 
                             reason: str, duration: float):
        """Track data promotion between memory tiers"""
        user_hash = hashlib.sha256(user_id.encode()).hexdigest()[:8]
        
        # Track the decision
        ai_agent_decisions_total.labels(
            decision_type="tier_promotion",
            from_tier=from_tier,
            to_tier=to_tier,
            reason=reason
        ).inc()
        
        # Track promotion duration
        memory_promotion_duration_seconds.labels(
            from_tier=from_tier,
            to_tier=to_tier
        ).observe(duration)
        
        # Update tier sizes
        memory_tier_size_bytes.labels(tier=to_tier).inc(size_bytes)
        memory_tier_size_bytes.labels(tier=from_tier).dec(size_bytes)
    
    def track_neuros_mode_switch(self, user_id: str, from_mode: str, 
                                to_mode: str, trigger: str):
        """Track NEUROS cognitive mode switches"""
        user_hash = hashlib.sha256(user_id.encode()).hexdigest()[:8]
        
        # Track the switch
        neuros_mode_switches_total.labels(
            from_mode=from_mode,
            to_mode=to_mode,
            trigger_type=trigger,
            user_id_hash=user_hash
        ).inc()
        
        # Update current mode
        neuros_current_mode.labels(user_id_hash=user_hash).state(to_mode)
        
        # Track duration in previous mode
        if user_hash in self.mode_start_times and from_mode != "unknown":
            duration = time.time() - self.mode_start_times[user_hash]
            neuros_mode_duration_seconds.labels(
                mode=from_mode,
                user_id_hash=user_hash
            ).observe(duration)
        
        # Record new mode start time
        self.mode_start_times[user_hash] = time.time()
    
    def track_biometric_event(self, device_type: str, event_type: str, 
                            user_id: str, metrics: Dict[str, Any],
                            quality_score: float = 1.0):
        """Track biometric event processing"""
        user_hash = hashlib.sha256(user_id.encode()).hexdigest()[:8]
        
        biometric_events_processed_total.labels(
            device_type=device_type,
            event_type=event_type,
            user_id_hash=user_hash
        ).inc()
        
        # Track metric values
        for metric_name, value in metrics.items():
            if isinstance(value, (int, float)):
                # Determine unit from metric name
                unit = self._infer_unit(metric_name)
                biometric_event_values.labels(
                    device_type=device_type,
                    metric_type=metric_name,
                    unit=unit
                ).observe(value)
        
        # Track data quality
        biometric_data_quality_score.labels(
            device_type=device_type,
            user_id_hash=user_hash
        ).set(quality_score)
    
    def track_kafka_message(self, topic: str, success: bool, 
                          duration: float, batch_size: int = 1):
        """Track Kafka message operations"""
        kafka_messages_sent_total.labels(
            topic=topic,
            status="success" if success else "failed"
        ).inc(batch_size)
        
        kafka_message_send_duration_seconds.labels(
            topic=topic
        ).observe(duration)
        
        if batch_size > 1:
            kafka_batch_size.labels(topic=topic).observe(batch_size)
    
    def track_hypothesis_generation(self, category: str, confidence: str):
        """Track NEUROS hypothesis generation"""
        neuros_hypothesis_generated_total.labels(
            category=category,
            confidence_level=confidence
        ).inc()
    
    def track_pattern_recognition(self, pattern_type: str, significance: str):
        """Track NEUROS pattern recognition"""
        neuros_pattern_recognition_total.labels(
            pattern_type=pattern_type,
            significance=significance
        ).inc()
    
    def update_memory_hit_rate(self, tier: str, hit_rate: float):
        """Update memory tier hit rate"""
        memory_tier_hit_rate.labels(tier=tier).set(hit_rate)
    
    def detect_anomaly(self, device_type: str, anomaly_type: str, 
                      severity: str = "low"):
        """Track detected anomalies"""
        biometric_anomalies_detected_total.labels(
            device_type=device_type,
            anomaly_type=anomaly_type,
            severity=severity
        ).inc()
    
    def _infer_unit(self, metric_name: str) -> str:
        """Infer unit from metric name"""
        metric_lower = metric_name.lower()
        if "heart" in metric_lower or "hr" in metric_lower:
            return "bpm"
        elif "temp" in metric_lower:
            return "celsius"
        elif "weight" in metric_lower:
            return "kg"
        elif "distance" in metric_lower:
            return "meters"
        elif "calorie" in metric_lower:
            return "kcal"
        elif "sleep" in metric_lower or "duration" in metric_lower:
            return "seconds"
        elif "percent" in metric_lower or "%" in metric_lower:
            return "percent"
        else:
            return "unit"

# Global metrics tracker instance
metrics_tracker = MetricsTracker()

# =============================================================================
# CONTEXT MANAGERS FOR METRICS
# =============================================================================

class WebhookMetrics:
    """Context manager for webhook metrics tracking"""
    
    def __init__(self, device_type: str, event_type: str = "unknown"):
        self.device_type = device_type
        self.event_type = event_type
        self.start_time = None
        self.payload_size = 0
    
    def __enter__(self):
        self.start_time = time.time()
        webhook_active_requests.labels(device_type=self.device_type).inc()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        webhook_active_requests.labels(device_type=self.device_type).dec()
        
        status = "error" if exc_type else "success"
        error_type = type(exc_val).__name__ if exc_val else None
        
        metrics_tracker.track_webhook_request(
            device_type=self.device_type,
            event_type=self.event_type,
            start_time=self.start_time,
            status=status,
            error=error_type,
            payload_size=self.payload_size
        )
        
        return False  # Don't suppress exceptions

class MemoryOperationMetrics:
    """Context manager for memory operation metrics"""
    
    def __init__(self, tier: str, operation: str, trigger: str = "manual"):
        self.tier = tier
        self.operation = operation
        self.trigger = trigger
        self.start_time = None
        self.size_bytes = None
        self.item_count = None
    
    def __enter__(self):
        self.start_time = time.time()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        duration = time.time() - self.start_time
        success = exc_type is None
        
        metrics_tracker.track_memory_operation(
            tier=self.tier,
            operation=self.operation,
            trigger=self.trigger,
            success=success,
            duration=duration,
            size_bytes=self.size_bytes,
            item_count=self.item_count
        )
        
        return False 